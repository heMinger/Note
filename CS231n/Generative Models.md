# Generative Models
# date: 12/11/24
writer: MingheLi

# Overview
1. Unsupervised Learning
2. Generative Models
- PixelRNN and PixelCNN
- Variantional Autoencoders(VAE)
- Generative Adversarial Networks(GAN)

# Unsupervised Learning
1. different with supervised learning: just data, no labels

# Generative Models
**given training data, try to generate new samples from the same distribution**
## focus
1. addresses density estimation, a core problem in unsupervised learning
2. several flavors:
- explicit density estimation: explicitly define and solve for $p_{model}(x)$
- implicit density estimation: learn model that can sample from $p_{model}(x)$
## Why generative models
1. realistic samples for artwork, super-resolution, colorization
2. generative model of time-series data can be used for simulation and planning, useful for reinforcement learning applicaitions.
3. training generative models can also enable inference of latent representations that can be useful as general features.
---------
![taxonomy of generative models](https://github.com/user-attachments/assets/47b1d6e9-688e-4ade-b2de-55053efb26ce)
## Pixel RNN and Pixel CNN
1. explicit density model
2. use an chain rule to decompose likelihood of an image x into product of 1-d distributions
- ![image](https://github.com/user-attachments/assets/4209510f-46eb-4f1f-9b05-1ab9dbaff3c3)
3. then maximize likelihood of training data
4. using a neural network to express this complex distribution over pixel values
### Pixel RNN
1. process
- generate image pixels starting from corner
- dependency on previous pixels modeled using an RNN(LSTM)
- ![image](https://github.com/user-attachments/assets/16a74acd-dbf8-4255-abec-bdaaca31e13f)
- drawback: sequential generation is slow
### Pixel CNN
1. still generate image pixels starting from corner
2. dependency on previous pixels now modeled using a CNN over context region.
- difference1: using CNN
- diffenence2: over context region rather than previous pixels (still pixels actuallyðŸ˜‚ neighbor pixels already generated)
- ![image](https://github.com/user-attachments/assets/63195a14-34f4-4b52-bcc5-8d09d4c1ae98)
3. training: maximize likelihood of training images
- $p(x)=\prod_{i=1}^{n}p(x_i|x_1, ...,x_{i-1})$
4. efficency:
- training: faster than PixleRNN, cause we can parallelize convolutions since context region values known from traning images.
- test: generation must still proceed sequentially
### summary
![image](https://github.com/user-attachments/assets/16157c81-859d-4c87-9d8c-0bcf45b901b3)

## Variational Autoencoders(VAE)
### Autoencoders
1. an unsupervised diemensional feature representation from unlabeled training data.
2. encoder is going to be a function mapping from input data x to feature z.
3. Q: Why dimensionality reduction?
- I guess: for computational complexity.
- ans: want z to be able to learn features that can capture meaningful factpors of variation in the data.
4. Q: How to learn this feature representation?
- train such that features can be used to reconstruct original data
- autoencoding: encoding itself
5. feature map generated by encoder could used to initialize a supervised model.
- for example, to initialize a classifier.
- significance: use a lot of unlabeled training data to try and learn good general feature representations.
6. intuition: features that we learned being able to capture factors of variation in the training data.
### Variational autoencoders
1. probibilistic spin on autoencoders that will let us sample from model in order to generate new data.
2. intuition: z is some vector capturing how little or how much of factor of variation that we have in out training data.
3. generation process: sample from a prior over Z, and generate our data X by sampling from a conditional distribution P of X given z.
- sample z first, sample a value for each of latent factors and then we'll use that and sample our image x from here.
4. 
