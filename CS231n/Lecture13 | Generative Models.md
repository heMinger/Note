# Generative Models
# date: 12/11/24
writer: MingheLi

# Overview
1. Unsupervised Learning
2. Generative Models
- PixelRNN and PixelCNN
- Variantional Autoencoders(VAE)
- Generative Adversarial Networks(GAN)

# Unsupervised Learning
1. different with supervised learning: just data, no labels

# Generative Models
**given training data, try to generate new samples from the same distribution**
## focus
1. addresses density estimation, a core problem in unsupervised learning
2. several flavors:
- explicit density estimation: explicitly define and solve for $p_{model}(x)$
- implicit density estimation: learn model that can sample from $p_{model}(x)$
## Why generative models
1. realistic samples for artwork, super-resolution, colorization
2. generative model of time-series data can be used for simulation and planning, useful for reinforcement learning applicaitions.
3. training generative models can also enable inference of latent representations that can be useful as general features.
---------
![taxonomy of generative models](https://github.com/user-attachments/assets/47b1d6e9-688e-4ade-b2de-55053efb26ce)
## Pixel RNN and Pixel CNN
1. explicit density model
2. use an chain rule to decompose likelihood of an image x into product of 1-d distributions
- ![image](https://github.com/user-attachments/assets/4209510f-46eb-4f1f-9b05-1ab9dbaff3c3)
3. then maximize likelihood of training data
4. using a neural network to express this complex distribution over pixel values
### Pixel RNN
1. process
- generate image pixels starting from corner
- dependency on previous pixels modeled using an RNN(LSTM)
- ![image](https://github.com/user-attachments/assets/16a74acd-dbf8-4255-abec-bdaaca31e13f)
- drawback: sequential generation is slow
### Pixel CNN
1. still generate image pixels starting from corner
2. dependency on previous pixels now modeled using a CNN over context region.
- difference1: using CNN
- diffenence2: over context region rather than previous pixels (still pixels actuallyðŸ˜‚ neighbor pixels already generated)
- ![image](https://github.com/user-attachments/assets/63195a14-34f4-4b52-bcc5-8d09d4c1ae98)
3. training: maximize likelihood of training images
- $p(x)=\prod_{i=1}^{n}p(x_i|x_1, ...,x_{i-1})$
4. efficency:
- training: faster than PixleRNN, cause we can parallelize convolutions since context region values known from traning images.
- test: generation must still proceed sequentially
### summary
![image](https://github.com/user-attachments/assets/16157c81-859d-4c87-9d8c-0bcf45b901b3)

## Variational Autoencoders(VAE)
### Autoencoders
1. an unsupervised diemensional feature representation from unlabeled training data.
2. encoder is going to be a function mapping from input data x to feature z.
3. Q: Why dimensionality reduction?
- I guess: for computational complexity.
- ans: want z to be able to learn features that can capture meaningful factpors of variation in the data.
4. Q: How to learn this feature representation?
- train such that features can be used to reconstruct original data
- autoencoding: encoding itself
5. feature map generated by encoder could used to initialize a supervised model.
- for example, to initialize a classifier.
- significance: use a lot of unlabeled training data to try and learn good general feature representations.
6. intuition: features that we learned being able to capture factors of variation in the training data.
### Variational autoencoders
1. probibilistic spin on autoencoders that will let us sample from model in order to generate new data.
2. intuition: z is some vector capturing how little or how much of factor of variation that we have in out training data.
3. generation process: sample from a prior over Z, and generate our data X by sampling from a conditional distribution P of X given z.
- sample z first, sample a value for each of latent factors and then we'll use that and sample our image x from here.
4. can't optimize directly, derive and optimize lower bound on likelihood instead
5. too mathematicalðŸ˜¥ðŸ˜¥
## GAN
1. don't work with any explicit density function. take game-theoretic approach: learn to generate from training distribution through 2-player game
2. Q: what can we use to represent this complex transformation?
A: A neural network! **when we want to model some kind of complex function or transformation we use a neural network.**
3. training:
- Generator network: try to fool the discriminator by generating real-looking images
- discriminator network: try to distinguish between real and fake images.
- minimax objective function: ![image](https://github.com/user-attachments/assets/c9433c16-b600-4f0b-9ff2-3257d865ca06)
- ![image](https://github.com/user-attachments/assets/cfe82fb2-8caf-4cbf-9282-5ecef0ec4ad4)
4. generator objective doesn't work that well in practice
- the reason is we have to look at the loss landscape
- ![image](https://github.com/user-attachments/assets/7e818e22-8f25-4f71-b08c-a78373117970)
- slope of this loss is going to be higher towards the right. (x axis closer to 1, graident is larger)
- when our generator is doing a good job of fooling the discriminator, D(G(z)) is close to one, we got a higher gradient
- when our generator generate some bad samples, D(G(z)) is close to zero region on the X axis, got a relative flat gradient.
- so, gradient signal is dominated by region where the sample is already pretty good. which is bad.
5. modify
- ![image](https://github.com/user-attachments/assets/0e33537f-deb9-466f-8b79-e84fa83bcff0)
- new objective: ![image](https://github.com/user-attachments/assets/3683cb2e-5d1f-4f66-bc25-42633929040b)
- 


